{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\aiden\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset structure verified.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"/ML/ubl vs non-ubl/data_vit\"\n",
    "\n",
    "# Check if directories exist\n",
    "assert os.path.exists(f\"{dataset_path}/train/images\"), \"Train images folder missing!\"\n",
    "assert os.path.exists(f\"{dataset_path}/train/labels\"), \"Train labels folder missing!\"\n",
    "assert os.path.exists(f\"{dataset_path}/valid/images\"), \"Validation images folder missing!\"\n",
    "assert os.path.exists(f\"{dataset_path}/valid/labels\"), \"Validation labels folder missing!\"\n",
    "assert os.path.exists(f\"{dataset_path}/test/images\"), \"Test images folder missing!\"\n",
    "\n",
    "print(\"✅ Dataset structure verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ data.yaml correctly updated.\n"
     ]
    }
   ],
   "source": [
    "data_yaml_path = f\"{dataset_path}/data.yaml\" #your images are directory\n",
    "\n",
    "# Correcting paths inside data.yaml\n",
    "with open(data_yaml_path, \"w\") as f:\n",
    "    f.write(f\"\"\"\n",
    "train: {dataset_path}/train/images\n",
    "val: {dataset_path}/valid/images\n",
    "test: {dataset_path}/test/images\n",
    "\n",
    "nc: 2\n",
    "names: ['non-ubl','ubl']\n",
    "    \"\"\")\n",
    "\n",
    "print(\"✅ data.yaml correctly updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "to_tensor = [Resize((144, 144)), ToTensor()]\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image, target\n",
    "\n",
    "def show_images(images, num_samples=40, cols=8):\n",
    "    \"\"\" Plots some samples from the dataset \"\"\"\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    # Calculate how many images to skip to get num_samples\n",
    "    total_images = len(images)\n",
    "    idx = int(total_images / num_samples)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        if i % idx == 0 and int(i/idx) < num_samples:  # Ensure we don't exceed num_samples\n",
    "            plt.subplot(int(num_samples/cols) + 1, cols, int(i/idx) + 1)\n",
    "            plt.imshow(to_pil_image(img[0]))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show() # This is needed to display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def to_pil_image(tensor):\n",
    "    return T.ToPILImage()(tensor)\n",
    "\n",
    "# Load YAML config\n",
    "with open(f\"{dataset_path}/data.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Get image paths (adjust based on your YAML structure)\n",
    "image_dir = f\"{dataset_path}/train\"  #your images are directory\n",
    "import os\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some images\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# Make sure read_image is working properly\n",
    "images = []\n",
    "for path in image_paths[:100]:  # Start with just 10 images for testing\n",
    "    try:\n",
    "        # Ensure path is a string\n",
    "        if isinstance(path, list):\n",
    "            path = path[0]  # Take first element if it's a list\n",
    "            \n",
    "        # Load with PIL first (more robust)\n",
    "        from PIL import Image\n",
    "        import torchvision.transforms as transforms\n",
    "        \n",
    "        pil_img = Image.open(path)\n",
    "        tensor_img = transforms.ToTensor()(pil_img)\n",
    "        images.append(tensor_img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "print(f\"Successfully loaded {len(images)} images\")\n",
    "\n",
    "# Show images\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images length: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Images length:\", len(images))\n",
    "if len(images) > 0:\n",
    "    print(\"First element type:\", type(images[0]))\n",
    "    if hasattr(images[0], \"__len__\"):\n",
    "        print(\"First element length:\", len(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run a quick test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m sample_datapoint = torch.unsqueeze(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m], \u001b[32m0\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitial shape: \u001b[39m\u001b[33m\"\u001b[39m, sample_datapoint.shape)\n\u001b[32m     25\u001b[39m embedding = PatchEmbedding()(sample_datapoint)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "#patching\n",
    "\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "# Run a quick test\n",
    "sample_datapoint = torch.unsqueeze(images[0][0], 0)\n",
    "print(\"Initial shape: \", sample_datapoint.shape)\n",
    "embedding = PatchEmbedding()(sample_datapoint)\n",
    "print(\"Patches shape: \", embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
